#Problem 1

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Load the dataset
data = pd.read_csv('D3.csv')
X = data.iloc[:, :3].values
y = data.iloc[:, 3].values


#Extract columns (X1, X2, X3) and dependent variable Y
X1 = data.iloc[:, 0].values
X2 = data.iloc[:, 1].values
X3 = data.iloc[:, 2].values
Y = data.iloc[:, 3].values

#Add a bias term (intercept) to each of the input variables
X1 = np.vstack((np.ones(len(X1)), X1)).T
X2 = np.vstack((np.ones(len(X2)), X2)).T
X3 = np.vstack((np.ones(len(X3)), X3)).T

#Define a function for gradient descent
def gradient_descent(X, Y, learning_rate=0.01, iterations=1000):
    m = len(Y)
    theta = np.zeros(X.shape[1])
    cost_history = []

    for i in range(iterations):
        #Compute predictions
        predictions = X.dot(theta)

        #Compute the cost (mean squared error)
        error = predictions - Y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        cost_history.append(cost)

        #Gradient calculation
        gradient = (1 / m) * X.T.dot(error)

        #Update the parameters (theta)
        theta = theta - learning_rate * gradient

    return theta, cost_history

#Define a function to plot the regression and the loss
def plot_results(X, Y, theta, cost_history, feature_name):
    #Plot the regression model
    plt.figure(figsize=(12, 5))

    #Plot the fitted line
    plt.subplot(1, 2, 1)
    plt.scatter(X[:, 1], Y, color='blue', label='Data Points')
    plt.plot(X[:, 1], X.dot(theta), color='red', label='Regression Line')
    plt.xlabel(f'{feature_name}')
    plt.ylabel('Y')
    plt.title(f'Regression Model: Y = {theta[0]:.2f} + {theta[1]:.2f} * {feature_name}')
    plt.legend()

    #Plot the loss over iterations
    plt.subplot(1, 2, 2)
    plt.plot(range(len(cost_history)), cost_history, color='purple')
    plt.xlabel('Iterations')
    plt.ylabel('Cost (Loss)')
    plt.title(f'Loss over Iterations for {feature_name}')

    plt.tight_layout()
    plt.show()

#Training for X1, X2, and X3 separately
learning_rate = 0.05
iterations = 1000

#Model for X1
theta_X1, cost_history_X1 = gradient_descent(X1, Y, learning_rate, iterations)
plot_results(X1, Y, theta_X1, cost_history_X1, 'X1')

#Model for X2
theta_X2, cost_history_X2 = gradient_descent(X2, Y, learning_rate, iterations)
plot_results(X2, Y, theta_X2, cost_history_X2, 'X2')

#Model for X3
theta_X3, cost_history_X3 = gradient_descent(X3, Y, learning_rate, iterations)
plot_results(X3, Y, theta_X3, cost_history_X3, 'X3')

#Compare final costs to determine which has lower loss
print(f"Final cost for X1: {cost_history_X1[-1]}")
print(f"Final cost for X2: {cost_history_X2[-1]}")
print(f"Final cost for X3: {cost_history_X3[-1]}")

#Report the models
print(f"Model for X1: Y = {theta_X1[0]:.4f} + {theta_X1[1]:.4f} * X1")
print(f"Model for X2: Y = {theta_X2[0]:.4f} + {theta_X2[1]:.4f} * X2")
print(f"Model for X3: Y = {theta_X3[0]:.4f} + {theta_X3[1]:.4f} * X3")

#Analyze the effect of different learning rates
learning_rates = [0.01, 0.05, 0.1]
for lr in learning_rates:
    theta_X1_lr, cost_history_X1_lr = gradient_descent(X1, Y, lr, iterations)
    print(f"Learning rate: {lr} | Final loss for X1: {cost_history_X1_lr[-1]} | Iterations: {len(cost_history_X1_lr)}")


#Problem 2

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Load the dataset
data = pd.read_csv('D3.csv')

#Extract columns (X1, X2, X3) and dependent variable Y
X1 = data.iloc[:, 0].values
X2 = data.iloc[:, 1].values
X3 = data.iloc[:, 2].values
Y = data.iloc[:, 3].values

#Combine X1, X2, X3 into a single matrix and add a bias term
X = np.vstack((np.ones(len(X1)), X1, X2, X3)).T

#Define a function for gradient descent
def gradient_descent_multi(X, Y, learning_rate=0.01, iterations=1000):
    m = len(Y)
    theta = np.zeros(X.shape[1])  # Initialize theta to zero
    cost_history = []

    for i in range(iterations):
        #Compute predictions
        predictions = X.dot(theta)
        
        #Compute the cost (mean squared error)
        error = predictions - Y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        cost_history.append(cost)
        
        #Gradient calculation
        gradient = (1 / m) * X.T.dot(error)
        
        #Update the parameters (theta)
        theta = theta - learning_rate * gradient
    
    return theta, cost_history

#Define a function to plot the loss
def plot_loss(cost_history):
    plt.figure(figsize=(8, 5))
    plt.plot(range(len(cost_history)), cost_history, color='purple')
    plt.xlabel('Iterations')
    plt.ylabel('Cost (Loss)')
    plt.title('Loss over Iterations')
    plt.show()

# Training using X1, X2, and X3 together
learning_rate = 0.05
iterations = 1000

theta_multi, cost_history_multi = gradient_descent_multi(X, Y, learning_rate, iterations)
plot_loss(cost_history_multi)

#Report the final model
print(f"Final linear model: Y = {theta_multi[0]:.4f} + {theta_multi[1]:.4f} * X1 + {theta_multi[2]:.4f} * X2 + {theta_multi[3]:.4f} * X3")
print(f"Final cost: {cost_history_multi[-1]}")

#Predict for new values
def predict(X_new, theta):
    X_new = np.hstack(([1], X_new))  # Add bias term
    return np.dot(X_new, theta)

#Predictions for new (X1, X2, X3) values
X_new_1 = [1, 1, 1]
X_new_2 = [2, 0, 4]
X_new_3 = [3, 2, 1]

y_pred_1 = predict(X_new_1, theta_multi)
y_pred_2 = predict(X_new_2, theta_multi)
y_pred_3 = predict(X_new_3, theta_multi)

print(f"Prediction for (X1=1, X2=1, X3=1): Y = {y_pred_1:.4f}")
print(f"Prediction for (X1=2, X2=0, X3=4): Y = {y_pred_2:.4f}")
print(f"Prediction for (X1=3, X2=2, X3=1): Y = {y_pred_3:.4f}")

#Analyze the effect of different learning rates
learning_rates = [0.01, 0.05, 0.1]
for lr in learning_rates:
    theta_lr, cost_history_lr = gradient_descent_multi(X, Y, lr, iterations)
    print(f"Learning rate: {lr} | Final loss: {cost_history_lr[-1]} | Iterations: {len(cost_history_lr)}")
